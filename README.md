# RAG (Retrieval-Augmented Generation) Chatbot Prototype

![DEMO](.github/assets/chatting_demo.gif)

_Read this in other languages: [Korean](README.ko.md)_

## Table of Contents
1. [Project Overview](#1-project-overview)
2. [Architecture](#2-architecture)
3. [Key Features](#3-key-features)
4. [Tech Stack](#4-tech-stack)
5. [Directory Structure](#5-directory-structure)
6. [Running Locally](#6-running-locally)
7. [Usage Guide](#7-usage-guide)

## 1. Project Overview

This project is a prototype of a Retrieval-Augmented Generation (RAG) based chatbot that can be run in a local environment. Users can input questions through a web interface, and the system retrieves relevant documents from a pre-built vector store. It then provides an answer generated by a Large Language Model (LLM) based on this information. It can also remember the context of conversations, enabling continuous dialogue.

The entire system follows a Microservice Architecture (MSA), where each function is separated into an independent service to facilitate easy maintenance and scalability.

## 2. Architecture

All services run independently, and the Client accesses backend functionalities exclusively through the Core API. The Core API acts as a gateway, communicating internally with the RAG Service to receive chatbot responses and forwarding them to the Client.

```mermaid
graph TD
    subgraph "User Interface"
        A[Client: React/Vite]
    end

    subgraph "Backend Services"
        B[Core API: Spring Boot/WebFlux]
        C[RAG Service: FastAPI/LangChain]
        D[Indexing Service: FastAPI]
    end

    subgraph "Data & AI Stores"
        E[Redis<br><i>Cache / Task Broker</i>]
        F[Local LLM: Ollama]
        G[Vector Store: Elasticsearch]
        H[Database: PostgreSQL/MongoDB]
    end

%% Main application flow
    A -- "HTTP API" --> B
    B -- "Internal API Call (Query + History)" --> C
    B -- "Triggers Indexing" --> D

%% Data store interactions
    B -- "Manages Metadata & Summary" --> H
    C -- "Retrieves data from" --> G
    D -- "Creates/Updates" --> G
    C -- "Generates response with" --> F

%% Redis-specific interactions
    B -- "Caches Sessions & Publishes Task Token" --> E
    C -- "Consumes Task Token" --> E
    D -- "Consumes Task Token" --> E
```

- **Client**: A web application where users interact with the chatbot and administrators manage documents. All requests are routed through the Core API.
- **Core API**: The main application server. It handles core logic such as user authentication (JWT), Role-Based Access Control (RBAC), and document management. It also serves as a gateway, routing client requests to the appropriate internal services. It also orchestrates conversation session management, including saving and managing conversation summaries in the database. It employs an in-memory cache (Caffeine) to implement an optimistic caching strategy for conversation summaries, enhancing responsiveness by providing immediate context during asynchronous processing.
- **RAG Service**:  Manages the core functionalities of the RAG pipeline, including communication with the LLM, vector store retrieval, and prompt generation. In addition to its core RAG functions, it provides a dedicated API for conversation summarization.
- **Indexing Service**: Receives requests from the Core API to convert source data into vectors and manages the creation of the vector store (Elasticsearch).
- **Local LLM**: Utilizes an LLM model via a locally installed Ollama instance.
- **Databases**: Consists of PostgreSQL and MongoDB used by the `core-api`, and an Elasticsearch vector store used by the `rag-service`.

## 3. Key Features

- **Real-time RAG Chatbot**: Provides answers generated by an LLM based on relevant documents retrieved in response to user questions.
- **Category-Based Filtered Search**: Users can specify a document category with their query, narrowing the search scope to a specific domain for more accurate and relevant answers.
- **User Authentication**: Secure login/logout functionality based on JWT.
- **Admin Dashboard**:
    - **Role-Based Access Control**: Accessible only to users with the ADMIN role.
    - **Document Management**: Allows uploading and deleting documents used for RAG.
    - **Prompt Management**: Allows administrators to create, update, and delete prompt templates used by the RAG Service.
- **Conversation Context Memory & Summarization**: Continuously remembers the dialogue with the user to provide context-aware answers. It asynchronously summarizes and manages the conversation through the collaboration of `core-api` and `rag-service`, ensuring stable handling of long dialogues.

## 4. Tech Stack

### 4.1. Frontend (`client`)
- **Framework**: React, Vite
- **Styling**: Tailwind CSS
- **State Management**: Zustand
- **Data Fetching**: React Query
- **Language**: TypeScript

### 4.2. Core API (`core-api`)
- **Framework**: Spring Boot 3 (with WebFlux)
- **Language**: Java 21
- **Authentication**: Spring Security, JWT
- **Database**: PostgreSQL (R2DBC), MongoDB (Reactive)
- **Build Tool**: Gradle

### 4.3. RAG Service (`rag-service`)
- **Framework**: FastAPI
- **Core Logic**: LangChain
- **Vector Store**: Elasticsearch
- **Language**: Python

### 4.4. Indexing Service (`indexing-service`)
- **Framework**: FastAPI
- **Language**: Python

### 4.5. AI & Infrastructure
- **LLM Engine**: Ollama
- **Containerization**: Docker, Docker Compose

## 5. Directory Structure

```
.
├── client/              # React Frontend
├── core-api/            # Spring Boot Backend API
├── indexing-service/    # Data vectorization and indexing service
├── rag-service/         # RAG and LLM integration service
├── docker-compose.yml   # Docker Compose settings for CPU environment
├── docker-compose.gpu.yml # Docker Compose override for GPU environment
└── README.md
```

## 6. Running Locally
This project can be easily run using `docker-compose`. You can choose to run it in either a CPU or GPU environment.

### 6.1. Common Prerequisites

1.  **Install Docker and Docker Compose**
    - Ensure Docker and Docker Compose are installed on your system.

2.  **Install Ollama and Download a Model**
    - Download and install Ollama from the[official website](https://ollama.com/).
    - Run the following command in your terminal to download an LLM model (e.g.,: `gpt-oss:2b`):
      ```bash
      ollama pull gpt-oss:20b
      ```
    - **Important**: The `rag-service` is configured by default to communicate with the local Ollama API at `http://host.docker.internal:11434`.

3. **Create Environment File**
   - In the project root directory, copy the `.env.example` file to create a `.env` file.
    ```bash
    cp .env.example .env
    ```
    - Modify the contents of the `.env` file to match your environment if necessary (e.g., `VOLUME_ROOT`).

### 6.2. How to Run (CPU Environment)
  - In the CPU environment, the `rag-service` and `indexing-service` will run on the CPU.
  - Run the following command in the project root to start all services in the background:
  ```bash
  docker compose up --build -d
  ```

### 6.3. How to Run (GPU Environment)

#### 6.3.1. GPU Prerequisites
  1. **NVIDIA GPU**: An NVIDIA GPU must be installed on your system.
  2. **NVIDIA 드라이버**: The latest NVIDIA drivers must be installed on the host machine.
  3. **NVIDIA Container Toolkit 설치**: You must install the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) to enable Docker containers to recognize and use the GPU.

#### 6.3.2. GPU Run Command
⚠️ **Important Notice for GPU Environment**

> The provided `docker-compose.gpu.yml` and `Dockerfile.gpu` are example configurations for a GPU environment. **They have not been thoroughly tested by the author in an actual GPU environment.** You may encounter unexpected issues, and the setup may not be compatible with all GPU models or driver versions.
>
> If you run into any problems, please consider opening an **[Issues](https://github.com/fake-dev-log/rag-chatbot-prototype/issues)** on the GitHub repository with details about the problem (e.g., your GPU model, driver version, error logs). Your feedback would be greatly appreciated.

  - Run the following command from the project root. This command merges the base `docker-compose.yml` with the `docker-compose.gpu.yml` override file to configure the GPU environment.
  ```bash
  docker compose -f docker-compose.yml -f docker-compose.gpu.yml up --build -d
  ```

### 6.4. Accessing Services
  - **Frontend**: `http://localhost:5173`
  - **Core API Docs**: `http://localhost:8080/docs`

### 6.5. Stopping Services
  - Run the following command to stop all running services and remove their resources:
  ```bash
  docker compose down
  ```

### 6.6. **Using the Ollama Container (Optional)**
If you prefer to manage Ollama as a Docker container instead of using a local installation, modify your `docker-compose.yml` file as follows.
  ```yaml
  services:
    ...
    rag-service:
      ...
      environment:
        # API keys, model names, environment variables, etc. if needed
        LLM_API_BASE: "http://llm-server:11434" # <- Uncomment this line
        # LLM_API_BASE: "http://host.docker.internal:11434" # <- Comment out this line
      ...
      depends_on:
        - llm-server # <- Uncomment this line
      ...
    llm-server:
      build:
        context: .
        dockerfile: Dockerfile.llm-server
      restart: always
      networks:
        internal-net:
          aliases:
            - llm-server
      expose:
        - "11434"
      volumes:
        - ollama-data:/root/.ollama
    ...
  
  volumes:
    ...
    # Uncomment the following section
    ollama-data:
      driver: local
      driver_opts:
        type: none
        o: bind
        device: ${VOLUME_ROOT}/ollama/data
    ...
  ```

## 7. Usage Guide

### 7.1. Sign In

![Sign-in Page](.github/assets/sign-in.png)

### 7.2. Main Page

- On the main screen, you can start a conversation immediately or review past conversations via 'Chat History'. ~~However, the LLM does not retain the context of past conversations, so you cannot continue a previous dialogue.~~
- The screenshot below shows the view for an administrator, which includes the 'Documents' and 'Prompts' menus. These menus are not accessible to regular users.
![New Chat](./.github/assets/new_chat.png)

#### Conversation Context Memory

- The chatbot now remembers the context of the conversation, so you can naturally continue the dialogue by asking follow-up questions.
![Memory](.github/assets/memory.png)

### 7.3. Document Management

- Admins can upload or delete documents to be used for RAG.
- When uploading a document, you can set its category.
![Documents](./.github/assets/documents_page.png)
- When querying the LLM, you can specify a document category to narrow the search scope, leading to more accurate and relevant answers.
![Select Category](./.github/assets/select_category.png)

### 7.4. Prompt Management

- Admins can create new prompts or modify/delete existing ones.
- By clicking the 'Apply' button, an admin can apply the selected prompt to the chatbot. This setting affects all users.
![Prompts](./.github/assets/prompts_page.png)

#### Default Template

- This is the default prompt written in the COSTAR (Context, Objective, Style, Tone, Audience, Response) format. It is used when no other prompt is applied.
![Default Template](./.github/assets/default_template.png)

#### Example Template

- This prompt is written with content related to medical device security to serve as a usage example.
![Example Template](./.github/assets/example_template.png)

### 7.5. Response Differences Based on Prompts

#### When 'Medical Device Security' Template is Applied

- This is a response generated with the 'Medical Device Security' template. As shown, it provides a relevant answer based on evidence from the reference documents.
![Medical Device Security Chat](./.github/assets/medical_device_security_chat.png)

- Clicking the Sources button shows which documents were searched and used.
![Source Documents](.github/assets/sources.png)

- However, as defined in the prompt (OBJECTIVE: Answer question based only on the given context...), it clearly states that it cannot answer irrelevant questions due to a lack of reference information.
![No context](./.github/assets/no_context.png)

#### When 'Default' Template is Applied

- In contrast, when the default template is applied, the chatbot responds correctly to the same question.
![Well Answered](./.github/assets/well_answered.png)
